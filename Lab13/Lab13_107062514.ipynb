{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "# load lines dictionary \n",
    "lines = open('dataset/chatbot/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "# load conversations\n",
    "convs = open('dataset/chatbot/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "print('\\n'.join(lines[:3]))\n",
    "print()\n",
    "print('\\n'.join(convs[:3]))\n",
    "\n",
    "lines_dict = {}\n",
    "for line in lines:\n",
    "    l = line.split(\" +++$+++ \")\n",
    "    lines_dict[l[0]] = l[-1]\n",
    "    \n",
    "convs_list = []\n",
    "for conv in convs:\n",
    "    convs_list.append(conv.split(\" +++$+++ \")[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\").split(','))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "過濾掉沒用的符號，切出每個conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs_list:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(lines_dict[conv[i]])\n",
    "        answers.append(lines_dict[conv[i+1]])\n",
    "\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用前後兩句組成questions和answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把一些縮寫或是口語用法替換成正式用法，並過濾掉沒有必要的符號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_line_length = 2\n",
    "max_line_length = 16\n",
    "\n",
    "def isNormal(l):\n",
    "    return len(l.split()) >= min_line_length and len(l.split()) <= max_line_length\n",
    "\n",
    "normal_questions = []\n",
    "normal_answers = []\n",
    "\n",
    "for question, answer in zip(clean_questions, clean_answers):\n",
    "    if isNormal(question) and isNormal(answer):\n",
    "        normal_questions.append(question)\n",
    "        normal_answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只取一個長度範圍內的句子，把太長或太短的字丟掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 39915\n",
      "Size of vocab we will use: 6662\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "for question in normal_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for answer in normal_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "threshold = 10\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1\n",
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算每個字出現的次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_question = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        vocab_question[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "vocab_answer = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        vocab_answer[word] = word_num\n",
    "        word_num += 1\n",
    "\n",
    "codes = ['<PAD>','<END>','<UNK>','<BEG>']\n",
    "\n",
    "for code in codes:\n",
    "    vocab_question[code] = len(vocab_question)+1\n",
    "    \n",
    "for code in codes:\n",
    "    vocab_answer[code] = len(vocab_answer)+1\n",
    "\n",
    "vocab_question_int = {v_i: v for v, v_i in vocab_question.items()}\n",
    "vocab_answer_int = {v_i: v for v, v_i in vocab_answer.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只把出現次數超過threshold的字加入vocabulary dictionary，避免train model時出現ResourceExausted。除此之外，在dictionary中加入PAD等後續會用到的符號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(normal_answers)):\n",
    "    normal_answers[i] = '<BEG> ' + normal_answers[i] + ' <END>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為answer在後面會當成decoder input 和 decoder output，所以在頭尾分別加入BEG和END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_int = []\n",
    "for question in normal_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in vocab_question:\n",
    "            ints.append(vocab_question['<UNK>'])\n",
    "        else:\n",
    "            ints.append(vocab_question[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in normal_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in vocab_answer:\n",
    "            ints.append(vocab_answer['<UNK>'])\n",
    "        else:\n",
    "            ints.append(vocab_answer[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果句子中出現dic中沒有的字，就用UNK代替"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_int_clean = []\n",
    "answers_int_clean = []\n",
    "\n",
    "for i in range(len(questions_int)):\n",
    "    if not(vocab_question['<UNK>'] in questions_int[i] or vocab_answer['<UNK>'] in answers_int[i]): # remove '<UNK>' sentence\n",
    "        questions_int_clean.append(questions_int[i])\n",
    "        answers_int_clean.append(answers_int[i])\n",
    "\n",
    "questions_int = questions_int_clean\n",
    "answers_int = answers_int_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只使用沒有UNK的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_line_length += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為answer的頭尾多加了BEG和END，所以最大長度要+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input\n",
      "Decoder input\n",
      "Decoder output\n",
      "\n",
      "not all experiences are good bianca you cannot always trust the people you want to <PAD> <PAD> <PAD>\n",
      "<BEG> i guess i will never know will i <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "i guess i will never know will i <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "you looked beautiful last night you know <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> so did you <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "so did you <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "let go <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> you set me up <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "you set me up <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "you set me up <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> i just wanted <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "i just wanted <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, questions, answers, batch_size):\n",
    "        assert len(questions) == len(answers)\n",
    "        \n",
    "        batch_num = len(questions)//batch_size\n",
    "        n = batch_num*batch_size\n",
    "        \n",
    "        self.xs = [np.zeros(n, dtype=np.int32) for _ in range(max_line_length)] # encoder inputs\n",
    "        self.ys = [np.zeros(n, dtype=np.int32) for _ in range(max_line_length)] # decoder inputs\n",
    "        self.gs = [np.zeros(n, dtype=np.int32) for _ in range(max_line_length)] # decoder outputs\n",
    "        self.ws = [np.zeros(n, dtype=np.float32) for _ in range(max_line_length)] # decoder weight for loss caculation\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for b in range(batch_num):\n",
    "            for i in range(b*batch_size, (b+1)*batch_size):\n",
    "                for j in range(len(questions[i])):\n",
    "                    self.xs[j][i] = questions[i][j]\n",
    "                for j in range(len(questions[i]), max_line_length):\n",
    "                    self.xs[j][i] = vocab_question['<PAD>']\n",
    "\n",
    "                for j in range(len(answers[i]) - 1):\n",
    "                    self.gs[j][i] = answers_int[i][j + 1]\n",
    "                    self.ys[j][i] = answers_int[i][j]\n",
    "                    self.ws[j][i] = 1.0\n",
    "                for j in range(len(answers[i]) - 1, max_line_length):\n",
    "                    self.gs[j][i] = vocab_answer['<PAD>']\n",
    "                    self.ys[j][i] = vocab_answer['<PAD>']\n",
    "                    self.ws[j][i] = 0.0\n",
    "    \n",
    "    def get(self, batch_id):\n",
    "        x = [self.xs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(max_line_length)]\n",
    "        y = [self.ys[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(max_line_length)]\n",
    "        g = [self.gs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(max_line_length)]\n",
    "        w = [self.ws[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(max_line_length)]\n",
    "        \n",
    "        return x, y, g, w\n",
    "\n",
    "    \n",
    "batch = BatchGenerator(questions_int, answers_int, 4)\n",
    "x, y, g, w = batch.get(7)\n",
    "print(\"Encoder input\")\n",
    "print(\"Decoder input\")\n",
    "print(\"Decoder output\")\n",
    "print()\n",
    "for i in range(4):\n",
    "    print(' '.join([vocab_question_int[x[j][i]] for j in range(max_line_length)]))\n",
    "    print(' '.join([vocab_answer_int[y[j][i]] for j in range(max_line_length)]))\n",
    "    print(' '.join([vocab_answer_int[g[j][i]] for j in range(max_line_length)]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook的BatchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Seq2Seq Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineTranslationSeq2Seq:\n",
    "    def __init__(self, en_max_len, ch_max_len, en_size, ch_size):\n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        \n",
    "        with tf.variable_scope('seq2seq_intput/output'):\n",
    "            self.enc_inputs = [tf.placeholder(tf.int32, [None]) for i in range(en_max_len)] # time mojor feed\n",
    "            self.dec_inputs = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.groundtruths = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.weights = [tf.placeholder(tf.float32, [None]) for i in range(ch_max_len)]\n",
    "            \n",
    "        with tf.variable_scope('seq2seq_rnn'): # training by teacher forcing\n",
    "            self.out_cell = tf.contrib.rnn.LSTMCell(512)\n",
    "            self.outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                    self.out_cell, \n",
    "                                                                                    en_size, ch_size, 300)\n",
    "        with tf.variable_scope('seq2seq_rnn', reuse=True): # predict by feeding previous\n",
    "            self.pred_cell = tf.contrib.rnn.LSTMCell(512, reuse=True) # reuse cell for train and test\n",
    "            self.predictions, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                        self.pred_cell, \n",
    "                                                                                        en_size, ch_size, 300, \n",
    "                                                                                        feed_previous=True)\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            # caculate weighted loss\n",
    "            self.loss = tf.reduce_mean(tf.contrib.legacy_seq2seq.sequence_loss_by_example(self.outputs, \n",
    "                                                                                          self.groundtruths, \n",
    "                                                                                          self.weights))\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.002).minimize(self.loss)\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train(self, x, y, g, w):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i] # show how to feed a list\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "            fd[self.groundtruths[i]] = g[i]\n",
    "            fd[self.weights[i]] = w[i]\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss, self.optimizer], fd)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def output(self, x, y):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "        \n",
    "        out = self.sess.run(self.outputs, fd)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, x, ch_beg):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len): # when feed previous, the fist token should be '<BEG>', and others are useless\n",
    "            if i==0:\n",
    "                fd[self.dec_inputs[i]] = np.ones(y[i].shape, dtype=np.int32)*ch_beg\n",
    "            else:\n",
    "                fd[self.dec_inputs[i]] = np.zeros(y[i].shape, dtype=np.int32)\n",
    "        \n",
    "        pd = self.sess.run(self.predictions, fd)\n",
    "        \n",
    "        return pd\n",
    "    \n",
    "    def save(self, e):\n",
    "        self.saver.save(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e+1))\n",
    "    \n",
    "    def restore(self, e):\n",
    "        self.saver.restore(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "batch_num = len(questions_int)//BATCH_SIZE\n",
    "\n",
    "batch = BatchGenerator(questions_int, answers_int, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = MachineTranslationSeq2Seq(max_line_length, max_line_length, len(vocab_question), len(vocab_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 4.889085\n",
      "epoch 1 loss: 4.218606\n",
      "epoch 2 loss: 3.951801\n",
      "epoch 3 loss: 3.694142\n",
      "epoch 4 loss: 3.420163\n",
      "epoch 5 loss: 3.158150\n",
      "epoch 6 loss: 2.917308\n",
      "epoch 7 loss: 2.696449\n",
      "epoch 8 loss: 2.488859\n",
      "epoch 9 loss: 2.314511\n",
      "epoch 10 loss: 2.161244\n",
      "epoch 11 loss: 2.022103\n",
      "epoch 12 loss: 1.906468\n",
      "epoch 13 loss: 1.793270\n",
      "epoch 14 loss: 1.684878\n",
      "epoch 15 loss: 1.593122\n",
      "epoch 16 loss: 1.512825\n",
      "epoch 17 loss: 1.449281\n",
      "epoch 18 loss: 1.383913\n",
      "epoch 19 loss: 1.337903\n",
      "epoch 20 loss: 1.294274\n",
      "epoch 21 loss: 1.266396\n",
      "epoch 22 loss: 1.216491\n",
      "epoch 23 loss: 1.177595\n",
      "epoch 24 loss: 1.139740\n",
      "epoch 25 loss: 1.104571\n",
      "epoch 26 loss: 1.083835\n",
      "epoch 27 loss: 1.061483\n",
      "epoch 28 loss: 1.036232\n",
      "epoch 29 loss: 1.006418\n",
      "epoch 30 loss: 0.990028\n",
      "epoch 31 loss: 0.966246\n",
      "epoch 32 loss: 0.951821\n",
      "epoch 33 loss: 0.930293\n",
      "epoch 34 loss: 0.899788\n",
      "epoch 35 loss: 0.877102\n",
      "epoch 36 loss: 0.872086\n",
      "epoch 37 loss: 0.873781\n",
      "epoch 38 loss: 0.865976\n",
      "epoch 39 loss: 0.839594\n",
      "epoch 40 loss: 0.804912\n",
      "epoch 41 loss: 0.793262\n",
      "epoch 42 loss: 0.790765\n",
      "epoch 43 loss: 0.784848\n",
      "epoch 44 loss: 0.779406\n",
      "epoch 45 loss: 0.770239\n",
      "epoch 46 loss: 0.752006\n",
      "epoch 47 loss: 0.738113\n",
      "epoch 48 loss: 0.727715\n",
      "epoch 49 loss: 0.715430\n",
      "epoch 50 loss: 0.697951\n",
      "epoch 51 loss: 0.702938\n",
      "epoch 52 loss: 0.700297\n",
      "epoch 53 loss: 0.700312\n",
      "epoch 54 loss: 0.696003\n",
      "epoch 55 loss: 0.675480\n",
      "epoch 56 loss: 0.663140\n",
      "epoch 57 loss: 0.659101\n",
      "epoch 58 loss: 0.659684\n",
      "epoch 59 loss: 0.656818\n",
      "epoch 60 loss: 0.649018\n",
      "epoch 61 loss: 0.640136\n",
      "epoch 62 loss: 0.631046\n",
      "epoch 63 loss: 0.623034\n",
      "epoch 64 loss: 0.619196\n",
      "epoch 65 loss: 0.615417\n",
      "epoch 66 loss: 0.614337\n",
      "epoch 67 loss: 0.614590\n",
      "epoch 68 loss: 0.609106\n",
      "epoch 69 loss: 0.626263\n",
      "epoch 70 loss: 0.609631\n",
      "epoch 71 loss: 0.593841\n",
      "epoch 72 loss: 0.590846\n",
      "epoch 73 loss: 0.582055\n",
      "epoch 74 loss: 0.571954\n",
      "epoch 75 loss: 0.563467\n",
      "epoch 76 loss: 0.563649\n",
      "epoch 77 loss: 0.566005\n",
      "epoch 78 loss: 0.570243\n",
      "epoch 79 loss: 0.558586\n",
      "epoch 80 loss: 0.554808\n",
      "epoch 81 loss: 0.561908\n",
      "epoch 82 loss: 0.566301\n",
      "epoch 83 loss: 0.557621\n",
      "epoch 84 loss: 0.551365\n",
      "epoch 85 loss: 0.543485\n",
      "epoch 86 loss: 0.539839\n",
      "epoch 87 loss: 0.531967\n",
      "epoch 88 loss: 0.543330\n",
      "epoch 89 loss: 0.557793\n",
      "epoch 90 loss: 0.545086\n",
      "epoch 91 loss: 0.537522\n",
      "epoch 92 loss: 0.538624\n",
      "epoch 93 loss: 0.526042\n",
      "epoch 94 loss: 0.531089\n",
      "epoch 95 loss: 0.528526\n",
      "epoch 96 loss: 0.535399\n",
      "epoch 97 loss: 0.541924\n",
      "epoch 98 loss: 0.550270\n",
      "epoch 99 loss: 0.542822\n"
     ]
    }
   ],
   "source": [
    "rec_loss = []\n",
    "for e in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for b in range(batch_num):\n",
    "        x, y, g, w = batch.get(b)\n",
    "        batch_loss = model.train(x, y, g, w)\n",
    "        train_loss += batch_loss\n",
    "    \n",
    "    train_loss /= batch_num\n",
    "    rec_loss.append(train_loss)\n",
    "    print(\"epoch %d loss: %f\" % (e, train_loss))\n",
    "    if (e+1)%10 == 0:\n",
    "        model.save(e)\n",
    "    \n",
    "np.save('./model/seq2seq/rec_loss.npy', rec_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def cherry_pick(records, n, upper_bound=1.0):\n",
    "    bleus = []\n",
    "    \n",
    "    for en, ch_gr, ch_pd in records:\n",
    "        # caculate BLEU by nltk\n",
    "        bleu = nltk.translate.bleu_score.sentence_bleu([ch_gr], ch_pd)\n",
    "        bleus.append(bleu)\n",
    "    \n",
    "    lst = [i for i in range(len(records)) if bleus[i]<=upper_bound]\n",
    "    lst = sorted(lst, key=lambda i: bleus[i], reverse=True) # sort by BLEU score\n",
    "    \n",
    "    return [records[lst[i]] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input\n",
      "Ground truth\n",
      "Decoder output\n",
      "\n",
      "goddamn it what kind of bullshit is that\n",
      "listen jake i gotta go\n",
      "listen jake i gotta go\n",
      "\n",
      "details baby details\n",
      "so they do know\n",
      "so they do know\n",
      "\n",
      "an excellent quality\n",
      "maybe he wants something\n",
      "maybe he wants something\n",
      "\n",
      "it depends on what way you want to go\n",
      "well wait you know what i know\n",
      "well wait you know what i know\n",
      "\n",
      "we have got some weather\n",
      "i noticed starck anybody home\n",
      "i noticed starck anybody home\n",
      "\n",
      "what did you expect lieutenant\n",
      "he is very human\n",
      "he is very human\n",
      "\n",
      "oh not at all i let them have twentyfive sure are not there four of them\n",
      "how fascinating do go on john\n",
      "how fascinating do go on john\n",
      "\n",
      "you think she is a saint\n",
      "she is been touched by god yes\n",
      "she is been touched by god yes\n",
      "\n",
      "josie you look rufus\n",
      "who are you guys\n",
      "who are you guys\n",
      "\n",
      "we have still got to find the mainframe\n",
      "no we do not\n",
      "no we do not\n",
      "\n",
      "will you bring it in to me\n",
      "i would rather not\n",
      "i would rather not\n",
      "\n",
      "can we go home soon rachel\n",
      "real soon jamie now shh\n",
      "soon jamie now shh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "\n",
    "records = []\n",
    "\n",
    "for i in range(10):\n",
    "    i = rd.randint(0, batch_num-1) # random pick one to translate\n",
    "    \n",
    "    x, y, g, w = batch.get(i)\n",
    "    out = model.output(x, y)\n",
    "    pd = model.predict(x, vocab_answer['<BEG>'])\n",
    "\n",
    "    for j in range(10):\n",
    "        j = rd.randint(0, BATCH_SIZE-1)\n",
    "        \n",
    "        en = [vocab_question_int[x[i][j]] for i in range(max_line_length)]\n",
    "        en = en[:en.index('<PAD>')]\n",
    "        ch_gr = [vocab_answer_int[g[i][j]] for i in range(max_line_length)]\n",
    "        if '<END>' in ch_gr:\n",
    "            ch_gr = ch_gr[:ch_gr.index('<END>')]\n",
    "        ch_pd = [vocab_answer_int[np.argmax(pd[i][j, :])] for i in range(max_line_length)]\n",
    "        if '<END>' in ch_pd:\n",
    "            ch_pd = ch_pd[:ch_pd.index('<END>')]\n",
    "        \n",
    "        records.append([en, ch_gr, ch_pd])\n",
    "\n",
    "n = 12 # how many result we show\n",
    "rec_cherry = cherry_pick(records, n)\n",
    "\n",
    "print(\"Encoder input\")\n",
    "print(\"Ground truth\")\n",
    "print(\"Decoder output\")\n",
    "print()\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(3):\n",
    "        print(' '.join(rec_cherry[i][j]))\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/seq2seq/seq2seq_90.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Hello.\n",
    "#How are you?\n",
    "#Where are you going?\n",
    "#You look great.\n",
    "#Good night.\n",
    "\n",
    "model.restore(90)\n",
    "q_test = ['hello', 'how are you', 'where are you going', 'you look great', 'good night']\n",
    "\n",
    "inf_x = [np.zeros(len(q_test), dtype=np.int32) for _ in range(max_line_length)]\n",
    "\n",
    "for i in range(len(q_test)):\n",
    "    q_list = q_test[i].split(' ')\n",
    "    last_num = 0\n",
    "    for j in range(len(q_list)):\n",
    "        inf_x[j][i] = vocab_question[q_list[j]]\n",
    "        last_num = j\n",
    "    for j in range(last_num + 1, max_line_length):\n",
    "        inf_x[j][i] = vocab_question['<PAD>']\n",
    "\n",
    "pd = model.predict(x, vocab_answer['<BEG>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello \n",
      "you will spend the night here \n",
      "\n",
      "how are you \n",
      "all right peter good night \n",
      "\n",
      "where are you going \n",
      "but it is my favorite state \n",
      "\n",
      "you look great \n",
      "god it is ugly it is ugly it is ugly it is ugly \n",
      "\n",
      "good night \n",
      "where is he \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for j in range(10):        \n",
    "        en = [en_rev[x[i][j]] for i in range(en_max_len)]\n",
    "        en = en[:en.index('<PAD>')]\n",
    "        ch_gr = [ch_rev[g[i][j]] for i in range(ch_max_len)]\n",
    "        if '<END>' in ch_gr:\n",
    "            ch_gr = ch_gr[:ch_gr.index('<END>')]\n",
    "        ch_pd = [ch_rev[np.argmax(pd[i][j, :])] for i in range(ch_max_len)]\n",
    "        if '<END>' in ch_pd:\n",
    "            ch_pd = ch_pd[:ch_pd.index('<END>')]\n",
    "        \n",
    "        records.append([en, ch_gr, ch_pd])\n",
    "'''        \n",
    "\n",
    "for j in range(len(q_test)):\n",
    "    qes = [vocab_question_int[inf_x[i][j]] for i in range(max_line_length)]\n",
    "    ans = [vocab_answer_int[np.argmax(pd[i][j, :])] for i in range(max_line_length)]\n",
    "    qes_str = ''\n",
    "    ans_str = ''\n",
    "    for q, a in zip(qes, ans):\n",
    "        if not q == '<PAD>':\n",
    "            qes_str += q + ' '\n",
    "        if not a == '<END>':\n",
    "            ans_str += a + ' '\n",
    "    print(qes_str)\n",
    "    print(ans_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model在training data的表現是好的，但是五句簡單的句子的回應卻不如日常對話，個人猜測有2個原因。首先，因為餵進model的training data是電影中的對話，很多對話並不像一般所認知的制式回答，而是跟特定場景或情節息息相關的，以這個角度來看的話，只要把5句話的回應套在特定場景的上下文中，也是能解釋得通的，例如：you look great這句的回應雖然並不是一般所認知的禮貌回答(如：thanks之類)，但是如果套進一個場景裡，這個場景是B剛剪完頭髮，B自己覺得很醜，而A看見B之後說you look great，而B說god it is ugly it is ugly it is ugly it is ugly就顯得合理了。其他的句子也能仿照這種方式解釋。第二，可能是餵進model的training data在處理時的問題，因為我們只單純的把前後兩句當成question和answer，並沒有考慮其他較複雜的狀況，例如：前面一句是整個話題的最後一句，而後面一句是另一個話題的第一句，沒有考慮到這些狀況會導致model可能把前後根本不相關的兩個句子當成是正確的question和answer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
