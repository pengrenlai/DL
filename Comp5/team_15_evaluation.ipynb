{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此我們的架構選用跟助教舊的版本(4類)，每個class隨機挑20000筆資料，總共湊成80000 training data，並做了up down flip的data augumentation，之後隨機挑example湊成一個batch，丟進CNN做training。Training 部分我們有用validation loss 來看是否有無overfitting，當loss和accuracy都開始震盪時，我們就停止training了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=8,suppress=True)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# libraries required for visualisation:\n",
    "from IPython.display import SVG, display\n",
    "import svgwrite # conda install -c omnia svgwrite=1.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to draw a sketch\n",
    "def get_bounds(data, factor=10):\n",
    "  \"\"\"Return bounds of data.\"\"\"\n",
    "  min_x = 0\n",
    "  max_x = 0\n",
    "  min_y = 0\n",
    "  max_y = 0\n",
    "\n",
    "  abs_x = 0\n",
    "  abs_y = 0\n",
    "  for i in range(len(data)):\n",
    "    x = float(data[i, 0]) / factor\n",
    "    y = float(data[i, 1]) / factor\n",
    "    abs_x += x\n",
    "    abs_y += y\n",
    "    min_x = min(min_x, abs_x)\n",
    "    min_y = min(min_y, abs_y)\n",
    "    max_x = max(max_x, abs_x)\n",
    "    max_y = max(max_y, abs_y)\n",
    "  return (min_x, max_x, min_y, max_y)\n",
    "\n",
    "\n",
    "def draw_strokes(data,\n",
    "                 svg_filename='/tmp/sketch_rnn/svg/sample.svg',\n",
    "                 factor=0.2,\n",
    "                 show_pen_sequence=False,\n",
    "                 who_draw_the_stroke=None):\n",
    "  if not os.path.exists(os.path.dirname(svg_filename)):\n",
    "    os.makedirs(os.path.dirname(svg_filename))\n",
    "  min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "  dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "  dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "  dwg.add(dwg.rect(insert=(0, 0), size=dims, fill='white'))\n",
    "  lift_pen = 1\n",
    "  abs_x = 25 - min_x\n",
    "  abs_y = 25 - min_y\n",
    "  p = \"M%s,%s \" % (abs_x, abs_y)\n",
    "  command = \"M\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for i in range(len(data)):\n",
    "    if (lift_pen == 1):\n",
    "      command = \"M\"\n",
    "    elif (command != \"L\"):\n",
    "      command = \"L\"\n",
    "    else:\n",
    "      command = \"\"\n",
    "    x = float(data[i, 0]) / factor\n",
    "    y = float(data[i, 1]) / factor\n",
    "    abs_x += x\n",
    "    abs_y += y\n",
    "    xs.append(abs_x)\n",
    "    ys.append(abs_y)\n",
    "    lift_pen = data[i, 2]\n",
    "    p += command + str(abs_x) + \",\" + str(abs_y) + \" \"\n",
    "  the_color = \"black\"\n",
    "  stroke_width = 1\n",
    "  dwg.add(dwg.path(p).stroke(the_color, stroke_width).fill(\"none\"))\n",
    "  color = 'black'\n",
    "  if show_pen_sequence:\n",
    "    turn = 0\n",
    "    for i in range(1, len(xs)):\n",
    "      dwg.add(\n",
    "          dwg.text(\n",
    "              '{}'.format(i),\n",
    "              insert=(xs[i], ys[i]),\n",
    "              font_size=\"10px\",\n",
    "              fill=color))\n",
    "      if who_draw_the_stroke is not None:\n",
    "        if data[i, 2] == 1:\n",
    "          color = 'red' if who_draw_the_stroke[turn] == 0 else 'black'\n",
    "          turn += 1\n",
    "  display(SVG(dwg.tostring()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train: 103938\n",
      "num valid: 2500\n",
      "num test: 2500\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = np.load('./dataset/microphone.full.npz', encoding='latin1')\n",
    "train_sketches = data['train']\n",
    "valid_sketches = data['valid']\n",
    "test_sketches = data['test']\n",
    "print('num train: {}'.format(len(train_sketches)))\n",
    "print('num valid: {}'.format(len(valid_sketches)))\n",
    "print('num test: {}'.format(len(test_sketches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -4   -6    0]\n",
      " [ -15   -1    0]\n",
      " [ -28   13    0]\n",
      " [ -10   18    0]\n",
      " [  -2   15    0]\n",
      " [  13   20    0]\n",
      " [  22    4    0]\n",
      " [   7   -2    0]\n",
      " [  12   -8    0]\n",
      " [   6   -9    0]\n",
      " [   0  -31    0]\n",
      " [  -6  -16    0]\n",
      " [  -7   -4    1]\n",
      " [ -38   13    0]\n",
      " [   5   37    0]\n",
      " [   6   18    1]\n",
      " [   1  -62    0]\n",
      " [  10   67    1]\n",
      " [   1  -73    0]\n",
      " [  13   67    0]\n",
      " [   5    8    1]\n",
      " [  -8  -68    0]\n",
      " [  12   47    1]\n",
      " [ -50  -20    0]\n",
      " [  24  -11    0]\n",
      " [  22   -6    1]\n",
      " [ -51   36    0]\n",
      " [  33  -14    0]\n",
      " [  21   -5    1]\n",
      " [ -39   25    0]\n",
      " [  27   -3    0]\n",
      " [  11   -8    1]\n",
      " [ -37   32    0]\n",
      " [   0   33    0]\n",
      " [   5    2    0]\n",
      " [  38   -7    0]\n",
      " [ -13  -32    1]\n",
      " [ -16    7    0]\n",
      " [   4   11    1]\n",
      " [-188  -38    0]\n",
      " [   0   20    0]\n",
      " [  -5   26    0]\n",
      " [   6    2    0]\n",
      " [  86   -7    0]\n",
      " [  38    1    0]\n",
      " [   2   -5    0]\n",
      " [  -8  -47    0]\n",
      " [ -18    6    0]\n",
      " [ -47    6    0]\n",
      " [ -51    3    0]\n",
      " [ -18   -1    1]\n",
      " [  57    0    0]\n",
      " [  -7   -9    0]\n",
      " [  -1   -8    0]\n",
      " [  12  -18    0]\n",
      " [   9   -2    0]\n",
      " [  11   10    0]\n",
      " [  10   32    1]\n",
      " [ -55   21    0]\n",
      " [   0    7    0]\n",
      " [   3    6    0]\n",
      " [   4    1    1]]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"625.0\" version=\"1.1\" width=\"1200.0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"625.0\" width=\"1200.0\" x=\"0\" y=\"0\"/><path d=\"M1170.0,60.0 M1150.0,30.0 L1075.0,25.0 935.0,90.0 L885.0,180.0 875.0,255.0 L940.0,355.0 1050.0,375.0 L1085.0,365.0 1145.0,325.0 L1175.0,280.0 1175.0,125.0 L1145.0,45.0 1110.0,25.0 M920.0,90.0 L945.0,275.0 975.0,365.0 M980.0,55.0 L1030.0,390.0 M1035.0,25.0 L1100.0,360.0 1125.0,400.0 M1085.0,60.0 L1145.0,295.0 M895.0,195.0 L1015.0,140.0 1125.0,110.0 M870.0,290.0 L1035.0,220.0 1140.0,195.0 M945.0,320.0 L1080.0,305.0 1135.0,265.0 M950.0,425.0 L950.0,590.0 975.0,600.0 L1165.0,565.0 1100.0,405.0 M1020.0,440.0 L1040.0,495.0 M100.0,305.0 L100.0,405.0 75.0,535.0 L105.0,545.0 535.0,510.0 L725.0,515.0 735.0,490.0 L695.0,255.0 605.0,285.0 L370.0,315.0 115.0,330.0 L25.0,325.0 M310.0,325.0 L275.0,280.0 270.0,240.0 L330.0,150.0 375.0,140.0 L430.0,190.0 480.0,350.0 M205.0,455.0 L205.0,490.0 220.0,520.0 L240.0,525.0 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/><text fill=\"black\" font-size=\"10px\" x=\"1075.0\" y=\"25.0\">1</text><text fill=\"black\" font-size=\"10px\" x=\"935.0\" y=\"90.0\">2</text><text fill=\"black\" font-size=\"10px\" x=\"885.0\" y=\"180.0\">3</text><text fill=\"black\" font-size=\"10px\" x=\"875.0\" y=\"255.0\">4</text><text fill=\"black\" font-size=\"10px\" x=\"940.0\" y=\"355.0\">5</text><text fill=\"black\" font-size=\"10px\" x=\"1050.0\" y=\"375.0\">6</text><text fill=\"black\" font-size=\"10px\" x=\"1085.0\" y=\"365.0\">7</text><text fill=\"black\" font-size=\"10px\" x=\"1145.0\" y=\"325.0\">8</text><text fill=\"black\" font-size=\"10px\" x=\"1175.0\" y=\"280.0\">9</text><text fill=\"black\" font-size=\"10px\" x=\"1175.0\" y=\"125.0\">10</text><text fill=\"black\" font-size=\"10px\" x=\"1145.0\" y=\"45.0\">11</text><text fill=\"black\" font-size=\"10px\" x=\"1110.0\" y=\"25.0\">12</text><text fill=\"black\" font-size=\"10px\" x=\"920.0\" y=\"90.0\">13</text><text fill=\"black\" font-size=\"10px\" x=\"945.0\" y=\"275.0\">14</text><text fill=\"black\" font-size=\"10px\" x=\"975.0\" y=\"365.0\">15</text><text fill=\"black\" font-size=\"10px\" x=\"980.0\" y=\"55.0\">16</text><text fill=\"black\" font-size=\"10px\" x=\"1030.0\" y=\"390.0\">17</text><text fill=\"black\" font-size=\"10px\" x=\"1035.0\" y=\"25.0\">18</text><text fill=\"black\" font-size=\"10px\" x=\"1100.0\" y=\"360.0\">19</text><text fill=\"black\" font-size=\"10px\" x=\"1125.0\" y=\"400.0\">20</text><text fill=\"black\" font-size=\"10px\" x=\"1085.0\" y=\"60.0\">21</text><text fill=\"black\" font-size=\"10px\" x=\"1145.0\" y=\"295.0\">22</text><text fill=\"black\" font-size=\"10px\" x=\"895.0\" y=\"195.0\">23</text><text fill=\"black\" font-size=\"10px\" x=\"1015.0\" y=\"140.0\">24</text><text fill=\"black\" font-size=\"10px\" x=\"1125.0\" y=\"110.0\">25</text><text fill=\"black\" font-size=\"10px\" x=\"870.0\" y=\"290.0\">26</text><text fill=\"black\" font-size=\"10px\" x=\"1035.0\" y=\"220.0\">27</text><text fill=\"black\" font-size=\"10px\" x=\"1140.0\" y=\"195.0\">28</text><text fill=\"black\" font-size=\"10px\" x=\"945.0\" y=\"320.0\">29</text><text fill=\"black\" font-size=\"10px\" x=\"1080.0\" y=\"305.0\">30</text><text fill=\"black\" font-size=\"10px\" x=\"1135.0\" y=\"265.0\">31</text><text fill=\"black\" font-size=\"10px\" x=\"950.0\" y=\"425.0\">32</text><text fill=\"black\" font-size=\"10px\" x=\"950.0\" y=\"590.0\">33</text><text fill=\"black\" font-size=\"10px\" x=\"975.0\" y=\"600.0\">34</text><text fill=\"black\" font-size=\"10px\" x=\"1165.0\" y=\"565.0\">35</text><text fill=\"black\" font-size=\"10px\" x=\"1100.0\" y=\"405.0\">36</text><text fill=\"black\" font-size=\"10px\" x=\"1020.0\" y=\"440.0\">37</text><text fill=\"black\" font-size=\"10px\" x=\"1040.0\" y=\"495.0\">38</text><text fill=\"black\" font-size=\"10px\" x=\"100.0\" y=\"305.0\">39</text><text fill=\"black\" font-size=\"10px\" x=\"100.0\" y=\"405.0\">40</text><text fill=\"black\" font-size=\"10px\" x=\"75.0\" y=\"535.0\">41</text><text fill=\"black\" font-size=\"10px\" x=\"105.0\" y=\"545.0\">42</text><text fill=\"black\" font-size=\"10px\" x=\"535.0\" y=\"510.0\">43</text><text fill=\"black\" font-size=\"10px\" x=\"725.0\" y=\"515.0\">44</text><text fill=\"black\" font-size=\"10px\" x=\"735.0\" y=\"490.0\">45</text><text fill=\"black\" font-size=\"10px\" x=\"695.0\" y=\"255.0\">46</text><text fill=\"black\" font-size=\"10px\" x=\"605.0\" y=\"285.0\">47</text><text fill=\"black\" font-size=\"10px\" x=\"370.0\" y=\"315.0\">48</text><text fill=\"black\" font-size=\"10px\" x=\"115.0\" y=\"330.0\">49</text><text fill=\"black\" font-size=\"10px\" x=\"25.0\" y=\"325.0\">50</text><text fill=\"black\" font-size=\"10px\" x=\"310.0\" y=\"325.0\">51</text><text fill=\"black\" font-size=\"10px\" x=\"275.0\" y=\"280.0\">52</text><text fill=\"black\" font-size=\"10px\" x=\"270.0\" y=\"240.0\">53</text><text fill=\"black\" font-size=\"10px\" x=\"330.0\" y=\"150.0\">54</text><text fill=\"black\" font-size=\"10px\" x=\"375.0\" y=\"140.0\">55</text><text fill=\"black\" font-size=\"10px\" x=\"430.0\" y=\"190.0\">56</text><text fill=\"black\" font-size=\"10px\" x=\"480.0\" y=\"350.0\">57</text><text fill=\"black\" font-size=\"10px\" x=\"205.0\" y=\"455.0\">58</text><text fill=\"black\" font-size=\"10px\" x=\"205.0\" y=\"490.0\">59</text><text fill=\"black\" font-size=\"10px\" x=\"220.0\" y=\"520.0\">60</text><text fill=\"black\" font-size=\"10px\" x=\"240.0\" y=\"525.0\">61</text></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see data format and draw it\n",
    "print(train_sketches[0])\n",
    "draw_strokes(train_sketches[0], show_pen_sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sketches, limit=1000):\n",
    "  raw_data = []\n",
    "  seq_len = []\n",
    "  count_data = 0\n",
    "\n",
    "  for i in range(len(sketches)):\n",
    "    data = sketches[i]\n",
    "    count_data += 1\n",
    "    # removes large gaps from the data\n",
    "    data = np.minimum(data, limit)\n",
    "    data = np.maximum(data, -limit)\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    raw_data.append(data)\n",
    "    seq_len.append(len(data))\n",
    "  seq_len = np.array(seq_len)  # nstrokes for each sketch\n",
    "  idx = np.argsort(seq_len)\n",
    "  sketches = []\n",
    "  for i in range(len(seq_len)):\n",
    "    sketches.append(raw_data[idx[i]])\n",
    "  return raw_data\n",
    "\n",
    "\n",
    "def normalize(sketches):\n",
    "  \"\"\"Normalize entire dataset (delta_x, delta_y) to [-1,1].\"\"\"\n",
    "  result = []\n",
    "  for i in range(len(sketches)):\n",
    "    tmp = sketches[i]\n",
    "    sketches[i][:, :2] /= np.max(np.abs(sketches[i][:, :2]))\n",
    "    result.append(tmp)\n",
    "  return result\n",
    "\n",
    "\n",
    "def to_big_strokes(strokes):\n",
    "  \"\"\"Converts from stroke-3 to stroke-5 format and pads to given length, but does not insert special start token).\"\"\"\n",
    "\n",
    "  result = np.zeros((len(strokes), 5), dtype=float)\n",
    "  l = len(strokes)\n",
    "  result[0:l, 0:2] = strokes[:, 0:2]\n",
    "  result[0:l, 3] = strokes[:, 2]\n",
    "  result[0:l, 2] = 1 - result[0:l, 3]\n",
    "  result[l:, 4] = 1\n",
    "  return result\n",
    "\n",
    "\n",
    "def to_big_sketches(sketches):\n",
    "  result = []\n",
    "  for i in range(len(sketches)):\n",
    "    sketch = to_big_strokes(sketches[i])\n",
    "    result.append(sketch)\n",
    "  return result\n",
    "\n",
    "\n",
    "def to_normal_strokes(big_strokes):\n",
    "  \"\"\"Convert from stroke-5 format (from sketch-rnn paper) back to stroke-3.\"\"\"\n",
    "  l = 0\n",
    "  for i in range(len(big_strokes)):\n",
    "    if big_strokes[i, 4] > 0:\n",
    "      l = i\n",
    "      break\n",
    "  if l == 0:\n",
    "    l = len(big_strokes)\n",
    "  result = np.zeros((l, 3))\n",
    "  result[:, 0:2] = big_strokes[0:l, 0:2]\n",
    "  result[:, 2] = big_strokes[0:l, 3]\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess sketches\n",
    "train_sketches = preprocess(train_sketches)\n",
    "valid_sketches = preprocess(valid_sketches)\n",
    "test_sketches = preprocess(test_sketches)\n",
    "\n",
    "train_sketches = normalize(train_sketches)\n",
    "valid_sketches = normalize(valid_sketches)\n",
    "test_sketches = normalize(test_sketches)\n",
    "\n",
    "# convert to stroke-5 format\n",
    "train_sketches = to_big_sketches(train_sketches)\n",
    "valid_sketches = to_big_sketches(valid_sketches)\n",
    "test_sketches = to_big_sketches(test_sketches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0212766  -0.03191489  0.        ]\n",
      " [-0.07978723 -0.00531915  0.        ]\n",
      " [-0.14893617  0.06914894  0.        ]\n",
      " [-0.05319149  0.09574468  0.        ]\n",
      " [-0.0106383   0.07978723  0.        ]\n",
      " [ 0.06914894  0.10638298  0.        ]\n",
      " [ 0.11702128  0.0212766   0.        ]\n",
      " [ 0.03723404 -0.0106383   0.        ]\n",
      " [ 0.06382979 -0.04255319  0.        ]\n",
      " [ 0.03191489 -0.04787234  0.        ]\n",
      " [ 0.         -0.16489361  0.        ]\n",
      " [-0.03191489 -0.08510638  0.        ]\n",
      " [-0.03723404 -0.0212766   1.        ]\n",
      " [-0.20212767  0.06914894  0.        ]\n",
      " [ 0.02659575  0.19680852  0.        ]\n",
      " [ 0.03191489  0.09574468  1.        ]\n",
      " [ 0.00531915 -0.32978722  0.        ]\n",
      " [ 0.05319149  0.35638297  1.        ]\n",
      " [ 0.00531915 -0.38829789  0.        ]\n",
      " [ 0.06914894  0.35638297  0.        ]\n",
      " [ 0.02659575  0.04255319  1.        ]\n",
      " [-0.04255319 -0.36170211  0.        ]\n",
      " [ 0.06382979  0.25        1.        ]\n",
      " [-0.26595744 -0.10638298  0.        ]\n",
      " [ 0.12765957 -0.05851064  0.        ]\n",
      " [ 0.11702128 -0.03191489  1.        ]\n",
      " [-0.27127659  0.19148937  0.        ]\n",
      " [ 0.17553191 -0.07446808  0.        ]\n",
      " [ 0.11170213 -0.02659575  1.        ]\n",
      " [-0.20744681  0.13297872  0.        ]\n",
      " [ 0.14361702 -0.01595745  0.        ]\n",
      " [ 0.05851064 -0.04255319  1.        ]\n",
      " [-0.19680852  0.17021276  0.        ]\n",
      " [ 0.          0.17553191  0.        ]\n",
      " [ 0.02659575  0.0106383   0.        ]\n",
      " [ 0.20212767 -0.03723404  0.        ]\n",
      " [-0.06914894 -0.17021276  1.        ]\n",
      " [-0.08510638  0.03723404  0.        ]\n",
      " [ 0.0212766   0.05851064  1.        ]\n",
      " [-1.         -0.20212767  0.        ]\n",
      " [ 0.          0.10638298  0.        ]\n",
      " [-0.02659575  0.13829787  0.        ]\n",
      " [ 0.03191489  0.0106383   0.        ]\n",
      " [ 0.45744681 -0.03723404  0.        ]\n",
      " [ 0.20212767  0.00531915  0.        ]\n",
      " [ 0.0106383  -0.02659575  0.        ]\n",
      " [-0.04255319 -0.25        0.        ]\n",
      " [-0.09574468  0.03191489  0.        ]\n",
      " [-0.25        0.03191489  0.        ]\n",
      " [-0.27127659  0.01595745  0.        ]\n",
      " [-0.09574468 -0.00531915  1.        ]\n",
      " [ 0.30319148  0.          0.        ]\n",
      " [-0.03723404 -0.04787234  0.        ]\n",
      " [-0.00531915 -0.04255319  0.        ]\n",
      " [ 0.06382979 -0.09574468  0.        ]\n",
      " [ 0.04787234 -0.0106383   0.        ]\n",
      " [ 0.05851064  0.05319149  0.        ]\n",
      " [ 0.05319149  0.17021276  1.        ]\n",
      " [-0.29255319  0.11170213  0.        ]\n",
      " [ 0.          0.03723404  0.        ]\n",
      " [ 0.01595745  0.03191489  0.        ]\n",
      " [ 0.0212766   0.00531915  1.        ]]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"53.058510618284345\" version=\"1.1\" width=\"56.11702127382159\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"53.058510618284345\" width=\"56.11702127382159\" x=\"0\" y=\"0\"/><path d=\"M31.090425509028137,25.186170211527497 M30.984042533673346,25.026595743838698 L30.585106373764575,25.0 29.840425536967814,25.34574467688799 L29.574468084611,25.824468098580837 29.521276596933603,26.22340425848961 L29.867021273821592,26.75531916320324 30.452127661556005,26.86170213855803 L30.638297870755196,26.808510650880635 30.957446806132793,26.595744700171053 L31.117021273821592,26.35638298932463 31.117021273821592,25.531914927996695 L30.957446806132793,25.106383026577532 30.771276596933603,25.00000005122274 M29.760638270527124,25.34574472811073 L29.893616996705532,26.32978731300682 30.05319146439433,26.808510734699667 M30.07978720823303,25.159574612043798 L30.345744660589844,26.941489442251623 M30.37234040442854,25.00000001396984 L30.71808508131653,26.781914844177663 30.85106380749494,26.994680794887245 M30.638297856785357,25.18617022316903 L30.957446792162955,26.43617022316903 M29.62765956763178,25.904255318455398 L30.265957438386977,25.61170212458819 30.85106382612139,25.452127656899393 M29.494680860079825,26.40957450028509 L30.37234040442854,26.03723408188671 30.930851050652564,25.9042553557083 M29.893616982735693,26.569148967973888 L30.611702078022063,26.48936173412949 30.90425527188927,26.276595783419907 M29.92021268699318,27.127659586258233 L29.92021268699318,28.00531913060695 30.05319141317159,28.058510618284345 L31.06382973957807,27.872340409085155 30.71808506269008,27.02127660624683 M30.292553161270916,27.20744681544602 L30.398936136625707,27.500000009313226 M25.398936136625707,26.489361682906747 L25.398936136625707,27.021276587620378 25.2659574104473,27.712765941396356 L25.4255318781361,27.76595742907375 27.71276594605297,27.57978721987456 L28.723404272459447,27.60638296371326 28.776595760136843,27.47340423753485 L28.56382980942726,26.22340423753485 28.085106387734413,26.38297870522365 L26.835106387734413,26.54255317291245 25.478723421692848,26.622340406756848 L25.0,26.59574466291815 M26.515957415103912,26.59574466291815 L26.329787205904722,26.356382952071726 26.303191462066025,26.143617001362145 L26.622340397443622,25.664893579669297 26.861702108290046,25.6117020919919 L27.154255302157253,25.877659544348717 27.42021275451407,26.728723347187042 M25.95744682243094,27.287233993411064 L25.95744682243094,27.473404202610254 26.037234056275338,27.632978670299053 L26.14361703163013,27.65957441413775 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(to_normal_strokes(train_sketches[0]))\n",
    "draw_strokes(to_normal_strokes(train_sketches[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_imgs(strokes, img_size, max_seq_len):\n",
    "  \"\"\" \n",
    "    convert 5-strokes format to image\n",
    "        args:\n",
    "            sketches: shape(data_size, max_seq_len, 5)\n",
    "    \"\"\"\n",
    "\n",
    "  xy = np.cumsum(strokes[:, :, 0:2], axis=1)  # (data_size, max_seq, 2)\n",
    "  min_xy = np.min(xy, axis=(1), keepdims=True)  # (data_size, 1, 2)\n",
    "  xy = xy - min_xy  # (data_size, max_seq, 2)\n",
    "  max_xy = np.max(xy, axis=(1), keepdims=True)  # (data_size, 1, 2)\n",
    "  max_xy = np.where(max_xy == 0, np.ones([len(strokes), 1, 2]),\n",
    "                    max_xy)  # avoid divide by 0\n",
    "  xy = xy / max_xy  # (data_size, max_seq, 2)\n",
    "  xy = xy * (img_size - 1)  # (data_size, max_seq, 2)\n",
    "\n",
    "  strokes_idx = np.tile(np.arange(len(strokes))[:, None],\n",
    "                        [1, img_size])  # (data_size, img_size)\n",
    "  interpolate_line = np.tile(\n",
    "      np.reshape(\n",
    "          np.arange(img_size).astype(np.float32) / (img_size - 1),\n",
    "          [1, img_size, 1]), [len(strokes), 1, 2])\n",
    "\n",
    "  def interpolate(p1, p2):\n",
    "    p1 = np.reshape(p1, [-1, 1, 2])\n",
    "    p2 = np.reshape(p2, [-1, 1, 2])\n",
    "    return (1 - interpolate_line\n",
    "           ) * p1 + interpolate_line * p2  # (data_size, img_size, 2)\n",
    "\n",
    "  images = np.zeros([len(strokes), img_size, img_size])\n",
    "  render_next = np.ones(len(images), dtype=np.bool)\n",
    "  for idx in range(max_seq_len - 1):\n",
    "    p1 = xy[:, idx]\n",
    "    p2 = xy[:, idx + 1]\n",
    "    # if p1 is connect to p2, draw a line between them\n",
    "    connect = np.where(\n",
    "        np.logical_and(strokes[:, idx, 3] > strokes[:, idx, 2],\n",
    "                       strokes[:, idx, 3] > strokes[:, idx, 4]),\n",
    "        np.zeros(len(images), dtype=np.bool),\n",
    "        np.ones(len(images), dtype=np.bool))\n",
    "\n",
    "    p_interpolate_line = interpolate(p1, p2).astype(\n",
    "        np.int32)  # (data_size, img_size, 2)\n",
    "    x_idx = np.where(connect[:, None], p_interpolate_line[:, :, 0],\n",
    "                     np.tile(xy[:, idx, None, 0], [1,\n",
    "                                                   img_size]).astype(np.int32))\n",
    "    y_idx = np.where(connect[:, None], p_interpolate_line[:, :, 1],\n",
    "                     np.tile(xy[:, idx, None, 1], [1,\n",
    "                                                   img_size]).astype(np.int32))\n",
    "    images[strokes_idx, x_idx, y_idx] = 1\n",
    "  images = np.rot90(images, -1, axes=(1, 2))\n",
    "  return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import random\n",
    "\n",
    "img_size = 64\n",
    "imgs = []\n",
    "labels = []\n",
    "valid_data_num = 2500\n",
    "\n",
    "for i in range(valid_data_num):\n",
    "    #random_index = random.randint(0, len(valid_sketches) - 1)\n",
    "    random_index = i\n",
    "    max_seq_len = len(valid_sketches[random_index])\n",
    "    rendered_imgs = render_imgs(np.array([valid_sketches[random_index]]), img_size, max_seq_len)\n",
    "    imgs.append(rendered_imgs[0])\n",
    "    labels.append(3) #balloon:0, bulb:1, ice:2, microphone:3\n",
    "    \n",
    "imgs = np.array(imgs) #(img_num, 64, 64) numpy array\n",
    "labels = np.array(labels)\n",
    "np.save(\"./train/valid_microphone_img_ren.npy\", imgs)\n",
    "np.save(\"./train/valid_microphone_label_ren.npy\", labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport random\\n\\nimg_size = 64\\nimgs = []\\nlabels = []\\n\\n\\nfor _ in range(train_data_num):\\n    random_index = random.randint(0, len(train_sketches) - 1)\\n    max_seq_len = len(train_sketches[random_index])\\n    rendered_imgs = render_imgs(np.array([train_sketches[random_index]]), img_size, max_seq_len)\\n    imgs.append(rendered_imgs[0])\\n    labels.append(0) #balloon:0, bulb:1, ice:2, microphone:3\\n    \\nimgs = np.array(imgs) #(img_num, 64, 64) numpy array\\nlabels = np.array(labels)\\nnp.save(\"./train/valid_balloon_img_ren.npy\", imgs)\\nnp.save(\"./train/valid_balloon_label_ren.npy\", labels)\\n'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_num = 20000\n",
    "\n",
    "'''\n",
    "import random\n",
    "\n",
    "img_size = 64\n",
    "imgs = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for _ in range(train_data_num):\n",
    "    random_index = random.randint(0, len(train_sketches) - 1)\n",
    "    max_seq_len = len(train_sketches[random_index])\n",
    "    rendered_imgs = render_imgs(np.array([train_sketches[random_index]]), img_size, max_seq_len)\n",
    "    imgs.append(rendered_imgs[0])\n",
    "    labels.append(0) #balloon:0, bulb:1, ice:2, microphone:3\n",
    "    \n",
    "imgs = np.array(imgs) #(img_num, 64, 64) numpy array\n",
    "labels = np.array(labels)\n",
    "np.save(\"./train/valid_balloon_img_ren.npy\", imgs)\n",
    "np.save(\"./train/valid_balloon_label_ren.npy\", labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "x_balloon_train = np.load(\"./train/train_balloon_img_ren.npy\")\n",
    "y_balloon_train = np.load(\"./train/train_balloon_label_ren.npy\")\n",
    "x_bulb_train = np.load(\"./train/train_bulb_img_ren.npy\")\n",
    "y_bulb_train = np.load(\"./train/train_bulb_label_ren.npy\")\n",
    "x_ice_train = np.load(\"./train/train_ice_img_ren.npy\")\n",
    "y_ice_train = np.load(\"./train/train_ice_label_ren.npy\")\n",
    "x_microphone_train = np.load(\"./train/train_microphone_img_ren.npy\")\n",
    "y_microphone_train = np.load(\"./train/train_microphone_label_ren.npy\")\n",
    "\n",
    "x_train_list = [x_balloon_train, x_bulb_train, x_ice_train, x_microphone_train]\n",
    "y_train_list = [y_balloon_train, y_bulb_train, y_ice_train, y_microphone_train]\n",
    "\n",
    "\n",
    "x_balloon_valid = np.load(\"./train/valid_balloon_img_ren.npy\")\n",
    "y_balloon_valid= np.load(\"./train/valid_balloon_label_ren.npy\")\n",
    "x_bulb_valid = np.load(\"./train/valid_bulb_img_ren.npy\")\n",
    "y_bulb_valid = np.load(\"./train/valid_bulb_label_ren.npy\")\n",
    "x_ice_valid = np.load(\"./train/valid_ice_img_ren.npy\")\n",
    "y_ice_valid = np.load(\"./train/valid_ice_label_ren.npy\")\n",
    "x_microphone_valid = np.load(\"./train/valid_microphone_img_ren.npy\")\n",
    "y_microphone_valid = np.load(\"./train/valid_microphone_label_ren.npy\")\n",
    "\n",
    "valid_x_train = np.vstack((x_balloon_valid, x_bulb_valid))\n",
    "valid_x_train = np.vstack((valid_x_train, x_ice_valid))\n",
    "valid_x_train = np.vstack((valid_x_train, x_microphone_valid))\n",
    "\n",
    "valid_y_train = np.hstack((y_balloon_valid, y_bulb_valid))\n",
    "valid_y_train = np.hstack((valid_y_train, y_ice_valid))\n",
    "valid_y_train = np.hstack((valid_y_train, y_microphone_valid))\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "valid_x_train , valid_y_train = shuffle(valid_x_train,valid_y_train,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "#return a batch of examples\n",
    "def get_next():\n",
    "    x_train = x_balloon_train[0]\n",
    "    y_train = y_balloon_train[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        type_num = random.randint(0, 3)\n",
    "        example_num = random.randint(0, train_data_num - 1)\n",
    "        if i == 0:\n",
    "            x_train = np.expand_dims(x_train_list[type_num][example_num], axis=0)\n",
    "            y_train = y_train_list[type_num][example_num]\n",
    "        else:\n",
    "            x_train = np.vstack((x_train, np.expand_dims(x_train_list[type_num][example_num], axis=0)))\n",
    "            y_train = np.hstack((y_train, y_train_list[type_num][example_num]))\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            x_train = np.fliplr(x_train)\n",
    "\n",
    "        \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate_model:\n",
    "\n",
    "  def __init__(self, image_size, model_name='evaluate_model'):\n",
    "    with tf.variable_scope(model_name, reuse=tf.AUTO_REUSE):\n",
    "      self.input_x = tf.placeholder(\n",
    "          tf.float32, [None, image_size, image_size], name=\"input_x\")\n",
    "      self.labels = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "      self.keep_rate = tf.placeholder(tf.float32, [], name=\"keep_rate\")\n",
    "\n",
    "      self.epoch = 100\n",
    "      self.model_name = model_name\n",
    "      self.batch_size = batch_size\n",
    "      self.total_data_num = train_data_num * 4\n",
    "    \n",
    "      self.global_step = tf.get_variable(\n",
    "        name='global_step', initializer=tf.constant(0), trainable=False)\n",
    "    \n",
    "      hidden = tf.reshape(self.input_x, [-1, image_size, image_size, 1])\n",
    "      hidden = tf.layers.conv2d(\n",
    "          hidden,\n",
    "          filters=64,\n",
    "          kernel_size=5,\n",
    "          activation=tf.nn.relu,\n",
    "          padding='same')\n",
    "      hidden = tf.nn.lrn(\n",
    "          hidden, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "      hidden = tf.layers.conv2d(\n",
    "          hidden,\n",
    "          filters=64,\n",
    "          kernel_size=5,\n",
    "          activation=tf.nn.relu,\n",
    "          padding='same')\n",
    "      hidden = tf.nn.lrn(\n",
    "          hidden, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "      hidden = tf.layers.max_pooling2d(\n",
    "          hidden, pool_size=3, strides=2, padding='same')\n",
    "      hidden = tf.layers.conv2d(\n",
    "          hidden,\n",
    "          filters=128,\n",
    "          kernel_size=5,\n",
    "          activation=tf.nn.relu,\n",
    "          padding='same')\n",
    "      hidden = tf.nn.lrn(\n",
    "          hidden, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm3')\n",
    "      hidden = tf.layers.conv2d(\n",
    "          hidden,\n",
    "          filters=128,\n",
    "          kernel_size=5,\n",
    "          activation=tf.nn.relu,\n",
    "          padding='same')\n",
    "      hidden = tf.nn.lrn(\n",
    "          hidden, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm4')\n",
    "      hidden = tf.layers.max_pooling2d(\n",
    "          hidden, pool_size=3, strides=2, padding='same')\n",
    "      hidden = tf.layers.flatten(hidden)\n",
    "      hidden = tf.layers.dense(hidden, 384)\n",
    "      hidden = tf.layers.dense(hidden, 192)\n",
    "      hidden = tf.layers.dense(hidden, 96)\n",
    "\n",
    "      # Add dropout\n",
    "      with tf.variable_scope(\"dropout\"):\n",
    "        hidden = tf.layers.dropout(hidden, 1 - self.keep_rate)\n",
    "      hidden = tf.layers.dense(hidden, 4)\n",
    "      # label of balloon: 0\n",
    "      # label of bulb: 1\n",
    "      # label of ice: 2\n",
    "      # label of microphone: 3\n",
    "      self.predictions = tf.nn.softmax(hidden)\n",
    "      argmax_prediction = tf.cast(tf.argmax(self.predictions, axis=1), tf.int32)\n",
    "\n",
    "      self.accuracy = tf.reduce_mean(\n",
    "          tf.to_float(tf.equal(argmax_prediction, self.labels)))\n",
    "\n",
    "      # CalculateMean cross-entropy loss\n",
    "      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          logits=hidden, labels=self.labels)\n",
    "      self.loss = tf.reduce_mean(losses)\n",
    "      for v in tf.trainable_variables():\n",
    "        self.loss += 0.001 * tf.nn.l2_loss(v)\n",
    "\n",
    "      # Define Training procedure\n",
    "      self.lr = tf.Variable(0.001, trainable=False)\n",
    "      optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "      grads_and_vars = optimizer.compute_gradients(\n",
    "          self.loss,\n",
    "          var_list=[v for v in tf.global_variables() if model_name in v.name])\n",
    "      self.train_op = optimizer.apply_gradients(\n",
    "          grads_and_vars, global_step=self.global_step)\n",
    "    \n",
    "  def train(self, sess):\n",
    "      _loss = []\n",
    "      num_batch_per_epoch = self.total_data_num//self.batch_size\n",
    "      train_loss_per_epoch = []\n",
    "      for e in range(self.epoch):\n",
    "        _loss = []\n",
    "        for _ in range(num_batch_per_epoch):\n",
    "            imgs, labels = get_next()\n",
    "            feed_dict = {\n",
    "                    self.input_x: imgs, \n",
    "                    self.labels: labels,\n",
    "                    self.keep_rate:1.0\n",
    "                }\n",
    "            loss, _ = sess.run([self.loss, self.train_op], feed_dict=feed_dict)\n",
    "            _loss.append(loss)\n",
    "        \n",
    "        print(\"epoch %d loss:   %.10f\" % (e, np.mean(_loss)))\n",
    "        valid_loss, valid_acc = sess.run([self.loss, self.accuracy],\n",
    "                                         feed_dict={self.input_x: valid_x_train[:500],\n",
    "                                                    self.labels: valid_y_train[:500]})\n",
    "        print(\"epoch %d valid loss:  %f ,   valid acc:  %f\" % (e, valid_loss, valid_acc))\n",
    "        if((e + 1) % 5 == 0):\n",
    "            self.save_model(sess, step=(e+1))\n",
    "        train_loss_per_epoch.append(np.mean(_loss))\n",
    "  \n",
    "  def save_model(self, sess, checkpoint_dir='./checkpoints_evaluation_update_ren', step=None):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "      os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(\n",
    "        tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.model_name +\n",
    "                          '/'))\n",
    "    if step is not None:\n",
    "      saver.save(\n",
    "          sess, os.path.join(checkpoint_dir, self.model_name), global_step=step)\n",
    "    else:\n",
    "      saver.save(sess, os.path.join(checkpoint_dir, self.model_name))\n",
    "    \n",
    "  def load_model(self, sess, checkpoint_dir='./checkpoints_evaluation_update_ren', step=None):\n",
    "    saver = tf.train.Saver(\n",
    "        tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.model_name +\n",
    "                          '/'))\n",
    "    if step is not None:\n",
    "      saver.restore(sess,\n",
    "                    os.path.join(checkpoint_dir,\n",
    "                                 self.model_name + '-{}'.format(step)))\n",
    "    else:\n",
    "      saver.restore(sess, os.path.join(checkpoint_dir, self.model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "evaluate_model = Evaluate_model(image_size=64, model_name='evaluate_model')\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss:   0.7836049795\n",
      "epoch 0 valid loss:  0.606634 ,   valid acc:  0.828000\n",
      "epoch 1 loss:   0.4766129851\n",
      "epoch 1 valid loss:  0.478547 ,   valid acc:  0.848000\n",
      "epoch 2 loss:   0.4133479893\n",
      "epoch 2 valid loss:  0.463624 ,   valid acc:  0.858000\n",
      "epoch 3 loss:   0.3811789751\n",
      "epoch 3 valid loss:  0.448545 ,   valid acc:  0.844000\n",
      "epoch 4 loss:   0.3670999706\n",
      "epoch 4 valid loss:  0.436085 ,   valid acc:  0.862000\n",
      "epoch 5 loss:   0.3511837423\n",
      "epoch 5 valid loss:  0.489881 ,   valid acc:  0.852000\n",
      "epoch 6 loss:   0.3468905389\n",
      "epoch 6 valid loss:  0.410802 ,   valid acc:  0.878000\n",
      "epoch 7 loss:   0.3399228156\n",
      "epoch 7 valid loss:  0.447310 ,   valid acc:  0.874000\n",
      "epoch 8 loss:   0.3303828239\n",
      "epoch 8 valid loss:  0.400268 ,   valid acc:  0.884000\n",
      "epoch 9 loss:   0.3253616393\n",
      "epoch 9 valid loss:  0.427875 ,   valid acc:  0.862000\n",
      "epoch 10 loss:   0.3242326081\n",
      "epoch 10 valid loss:  0.409309 ,   valid acc:  0.876000\n",
      "epoch 11 loss:   0.3233137727\n",
      "epoch 11 valid loss:  0.390512 ,   valid acc:  0.890000\n",
      "epoch 12 loss:   0.3262465298\n",
      "epoch 12 valid loss:  0.398889 ,   valid acc:  0.894000\n",
      "epoch 13 loss:   0.3243473768\n",
      "epoch 13 valid loss:  0.454330 ,   valid acc:  0.878000\n",
      "epoch 14 loss:   0.3158102632\n",
      "epoch 14 valid loss:  0.373894 ,   valid acc:  0.898000\n",
      "epoch 15 loss:   0.3149966300\n",
      "epoch 15 valid loss:  0.386060 ,   valid acc:  0.900000\n",
      "epoch 16 loss:   0.3106828332\n",
      "epoch 16 valid loss:  0.391443 ,   valid acc:  0.890000\n",
      "epoch 17 loss:   0.3108446598\n",
      "epoch 17 valid loss:  0.379462 ,   valid acc:  0.892000\n",
      "epoch 18 loss:   0.3109865189\n",
      "epoch 18 valid loss:  0.383533 ,   valid acc:  0.888000\n",
      "epoch 19 loss:   0.3109065890\n",
      "epoch 19 valid loss:  0.390917 ,   valid acc:  0.878000\n",
      "epoch 20 loss:   0.3078562319\n",
      "epoch 20 valid loss:  0.374033 ,   valid acc:  0.892000\n",
      "epoch 21 loss:   0.3048324287\n",
      "epoch 21 valid loss:  0.402899 ,   valid acc:  0.878000\n",
      "epoch 22 loss:   0.3059129715\n",
      "epoch 22 valid loss:  0.376078 ,   valid acc:  0.898000\n",
      "epoch 23 loss:   0.3057529926\n",
      "epoch 23 valid loss:  0.384556 ,   valid acc:  0.902000\n",
      "epoch 24 loss:   0.3012860119\n",
      "epoch 24 valid loss:  0.395545 ,   valid acc:  0.896000\n",
      "epoch 25 loss:   0.2981499434\n",
      "epoch 25 valid loss:  0.399019 ,   valid acc:  0.880000\n",
      "epoch 26 loss:   0.2966086864\n",
      "epoch 26 valid loss:  0.399390 ,   valid acc:  0.876000\n",
      "epoch 27 loss:   0.3017913699\n",
      "epoch 27 valid loss:  0.391075 ,   valid acc:  0.884000\n",
      "epoch 28 loss:   0.2950472534\n",
      "epoch 28 valid loss:  0.393345 ,   valid acc:  0.884000\n",
      "epoch 29 loss:   0.2991595268\n",
      "epoch 29 valid loss:  0.400298 ,   valid acc:  0.888000\n",
      "epoch 30 loss:   0.2959692776\n",
      "epoch 30 valid loss:  0.391402 ,   valid acc:  0.888000\n",
      "epoch 31 loss:   0.2964177132\n",
      "epoch 31 valid loss:  0.365155 ,   valid acc:  0.892000\n",
      "epoch 32 loss:   0.2897943258\n",
      "epoch 32 valid loss:  0.421405 ,   valid acc:  0.864000\n",
      "epoch 33 loss:   0.2953774035\n",
      "epoch 33 valid loss:  0.373768 ,   valid acc:  0.904000\n",
      "epoch 34 loss:   0.2946206033\n",
      "epoch 34 valid loss:  0.369470 ,   valid acc:  0.890000\n",
      "epoch 35 loss:   0.2931604683\n",
      "epoch 35 valid loss:  0.406962 ,   valid acc:  0.876000\n",
      "epoch 36 loss:   0.2922567427\n",
      "epoch 36 valid loss:  0.380258 ,   valid acc:  0.880000\n",
      "epoch 37 loss:   0.2934134305\n",
      "epoch 37 valid loss:  0.374189 ,   valid acc:  0.896000\n",
      "epoch 38 loss:   0.2889124751\n",
      "epoch 38 valid loss:  0.376064 ,   valid acc:  0.900000\n",
      "epoch 39 loss:   0.2943971753\n",
      "epoch 39 valid loss:  0.372584 ,   valid acc:  0.886000\n",
      "epoch 40 loss:   0.2944504023\n",
      "epoch 40 valid loss:  0.379148 ,   valid acc:  0.908000\n",
      "epoch 41 loss:   0.2931123972\n",
      "epoch 41 valid loss:  0.355693 ,   valid acc:  0.900000\n",
      "epoch 42 loss:   0.2874736488\n",
      "epoch 42 valid loss:  0.379406 ,   valid acc:  0.894000\n",
      "epoch 43 loss:   0.2949231565\n",
      "epoch 43 valid loss:  0.372763 ,   valid acc:  0.902000\n",
      "epoch 44 loss:   0.2896662056\n",
      "epoch 44 valid loss:  0.390288 ,   valid acc:  0.894000\n",
      "epoch 45 loss:   0.2894514501\n",
      "epoch 45 valid loss:  0.361410 ,   valid acc:  0.902000\n",
      "epoch 46 loss:   0.2908003032\n",
      "epoch 46 valid loss:  0.361521 ,   valid acc:  0.910000\n",
      "epoch 47 loss:   0.2878880799\n",
      "epoch 47 valid loss:  0.415328 ,   valid acc:  0.892000\n",
      "epoch 48 loss:   0.2829802334\n",
      "epoch 48 valid loss:  0.345408 ,   valid acc:  0.886000\n",
      "epoch 49 loss:   0.2869226635\n",
      "epoch 49 valid loss:  0.371122 ,   valid acc:  0.886000\n",
      "epoch 50 loss:   0.2883886099\n",
      "epoch 50 valid loss:  0.349358 ,   valid acc:  0.894000\n",
      "epoch 51 loss:   0.2829966545\n",
      "epoch 51 valid loss:  0.390619 ,   valid acc:  0.898000\n",
      "epoch 52 loss:   0.2860297859\n",
      "epoch 52 valid loss:  0.365355 ,   valid acc:  0.890000\n",
      "epoch 53 loss:   0.2850829959\n",
      "epoch 53 valid loss:  0.391286 ,   valid acc:  0.896000\n",
      "epoch 54 loss:   0.2832496464\n",
      "epoch 54 valid loss:  0.379831 ,   valid acc:  0.890000\n",
      "epoch 55 loss:   0.2842584848\n",
      "epoch 55 valid loss:  0.404601 ,   valid acc:  0.894000\n",
      "epoch 56 loss:   0.2838540971\n",
      "epoch 56 valid loss:  0.396722 ,   valid acc:  0.892000\n",
      "epoch 57 loss:   0.2843046784\n",
      "epoch 57 valid loss:  0.374235 ,   valid acc:  0.894000\n",
      "epoch 58 loss:   0.2802529335\n",
      "epoch 58 valid loss:  0.374226 ,   valid acc:  0.900000\n",
      "epoch 59 loss:   0.2845223546\n",
      "epoch 59 valid loss:  0.401545 ,   valid acc:  0.890000\n",
      "epoch 60 loss:   0.2839614153\n",
      "epoch 60 valid loss:  0.350883 ,   valid acc:  0.906000\n",
      "epoch 61 loss:   0.2805691063\n",
      "epoch 61 valid loss:  0.375222 ,   valid acc:  0.904000\n",
      "epoch 62 loss:   0.2777243853\n",
      "epoch 62 valid loss:  0.397218 ,   valid acc:  0.894000\n",
      "epoch 63 loss:   0.2812492251\n",
      "epoch 63 valid loss:  0.408805 ,   valid acc:  0.888000\n",
      "epoch 64 loss:   0.2857643962\n",
      "epoch 64 valid loss:  0.385360 ,   valid acc:  0.908000\n",
      "epoch 65 loss:   0.2874940336\n",
      "epoch 65 valid loss:  0.374685 ,   valid acc:  0.908000\n",
      "epoch 66 loss:   0.2829135358\n",
      "epoch 66 valid loss:  0.382488 ,   valid acc:  0.894000\n",
      "epoch 67 loss:   0.2800210416\n",
      "epoch 67 valid loss:  0.391605 ,   valid acc:  0.892000\n",
      "epoch 68 loss:   0.2844049335\n",
      "epoch 68 valid loss:  0.360754 ,   valid acc:  0.898000\n",
      "epoch 69 loss:   0.2820686102\n",
      "epoch 69 valid loss:  0.380863 ,   valid acc:  0.898000\n",
      "epoch 70 loss:   0.2839423418\n",
      "epoch 70 valid loss:  0.377530 ,   valid acc:  0.894000\n",
      "epoch 71 loss:   0.2853505909\n",
      "epoch 71 valid loss:  0.377148 ,   valid acc:  0.894000\n",
      "epoch 72 loss:   0.2784371078\n",
      "epoch 72 valid loss:  0.377009 ,   valid acc:  0.902000\n",
      "epoch 73 loss:   0.2791624367\n",
      "epoch 73 valid loss:  0.361621 ,   valid acc:  0.908000\n",
      "epoch 74 loss:   0.2794883549\n",
      "epoch 74 valid loss:  0.373889 ,   valid acc:  0.892000\n",
      "epoch 75 loss:   0.2756982148\n",
      "epoch 75 valid loss:  0.394730 ,   valid acc:  0.888000\n",
      "epoch 76 loss:   0.2834980488\n",
      "epoch 76 valid loss:  0.371809 ,   valid acc:  0.902000\n",
      "epoch 77 loss:   0.2780905664\n",
      "epoch 77 valid loss:  0.364979 ,   valid acc:  0.910000\n",
      "epoch 78 loss:   0.2836262286\n",
      "epoch 78 valid loss:  0.360249 ,   valid acc:  0.908000\n",
      "epoch 79 loss:   0.2793093026\n",
      "epoch 79 valid loss:  0.350493 ,   valid acc:  0.908000\n",
      "epoch 80 loss:   0.2776510417\n",
      "epoch 80 valid loss:  0.420589 ,   valid acc:  0.868000\n",
      "epoch 81 loss:   0.2773450613\n",
      "epoch 81 valid loss:  0.365526 ,   valid acc:  0.896000\n",
      "epoch 82 loss:   0.2819726467\n",
      "epoch 82 valid loss:  0.362806 ,   valid acc:  0.902000\n",
      "epoch 83 loss:   0.2831096649\n",
      "epoch 83 valid loss:  0.358211 ,   valid acc:  0.902000\n",
      "epoch 84 loss:   0.2777198851\n",
      "epoch 84 valid loss:  0.376955 ,   valid acc:  0.898000\n",
      "epoch 85 loss:   0.2823028266\n",
      "epoch 85 valid loss:  0.418780 ,   valid acc:  0.886000\n",
      "epoch 86 loss:   0.2819899917\n",
      "epoch 86 valid loss:  0.388426 ,   valid acc:  0.894000\n",
      "epoch 87 loss:   0.2789421678\n",
      "epoch 87 valid loss:  0.361626 ,   valid acc:  0.910000\n",
      "epoch 88 loss:   0.2813871503\n",
      "epoch 88 valid loss:  0.385584 ,   valid acc:  0.908000\n",
      "epoch 89 loss:   0.2777135372\n",
      "epoch 89 valid loss:  0.389738 ,   valid acc:  0.912000\n",
      "epoch 90 loss:   0.2810889781\n",
      "epoch 90 valid loss:  0.402948 ,   valid acc:  0.904000\n",
      "epoch 91 loss:   0.2788727880\n",
      "epoch 91 valid loss:  0.365693 ,   valid acc:  0.908000\n",
      "epoch 92 loss:   0.2751076221\n",
      "epoch 92 valid loss:  0.398187 ,   valid acc:  0.894000\n",
      "epoch 93 loss:   0.2764217854\n",
      "epoch 93 valid loss:  0.389975 ,   valid acc:  0.892000\n",
      "epoch 94 loss:   0.2794930041\n",
      "epoch 94 valid loss:  0.388778 ,   valid acc:  0.900000\n",
      "epoch 95 loss:   0.2751878798\n",
      "epoch 95 valid loss:  0.396599 ,   valid acc:  0.898000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96 loss:   0.2783615589\n",
      "epoch 96 valid loss:  0.355697 ,   valid acc:  0.910000\n",
      "epoch 97 loss:   0.2804755270\n",
      "epoch 97 valid loss:  0.369719 ,   valid acc:  0.892000\n",
      "epoch 98 loss:   0.2752047479\n",
      "epoch 98 valid loss:  0.402261 ,   valid acc:  0.898000\n",
      "epoch 99 loss:   0.2768175006\n",
      "epoch 99 valid loss:  0.365283 ,   valid acc:  0.886000\n"
     ]
    }
   ],
   "source": [
    "evaluate_model.train(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
